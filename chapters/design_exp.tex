\chapter{Design Evaluation Experiment}
\label{chap:de_exp}

\section{Introduction}

The final experiment combines the lessons of the previous experiment to investigate the use of the Rapidly Reconfigurable Research Cockpit (R3C) in a design evaluation study.
The goal of this experiment is to determine if the R3C system can be used in the place of a more traditional evaluation tool.
As previous chapters have discussed, there are a number of self-evident advantages to using the R3C system.
However, there remain some technical limitations to the technology that could hinder adoption.
We found that a button targeting task took more time in our virtual environment than in the real world (Chapter \ref{chap:pointing}).
The following experiment (Chapter \ref{chap:ph_exp}) found that a Fitts' Law task produced a higher throughput using a passive haptics layer, mitigating some of the time increase of targeting buttons in a virtual environment.
In the experiment described in this chapter, we used the R3C system as the simulation tool for a design evaluation study of a cockpit instrument.
The purpose in undergoing this evaluation study is to understand if these limitations interfere with the metrics used in evaluating a new cockpit design.

We designed an experiment which asks subjects for feedback on the design of a cockpit instrument.
The subjects were divided into two groups.
One group used an R3C setup to operate the instrument, while the other used a more traditional touchscreen simulator of the instrument without a head-mounted display.
This separation of groups will allow a comparison of the feedback from subjects between groups.
Both groups evaluated the same two instrument designs, and subjects were asked to provide feedback using the same questionnaires.
We hypothesize that the R3C system could be used in place of a traditional simulator if the two groups provide similar responses to the designs.
Additionally, we utilized common quantitative metrics to evaluate performance in each group.
The quantitative measures may change in magnitude between groups, but we need to determine if changes between designs are the same between groups.
%to determine if the conclusions that would be drawn from these change between groups.

\section{Methods}

For the mock design study we needed to define a task that the subjects could perform, that was analogous to flight tasks, and could be presented in two different designs for the comparison.
We chose to have subjects fly a one dimensional flight simulator using a joystick and use their other hand to input text on an instrument.
This is analogous to a common cockpit task where a pilot may have to enter information on an instrument (e.g.\ navigational waypoints or autopilots commands) while maintaining a flight path.
The difference in the instrument design will primarily affect the text input task, as will be described below in the description of the designs.
However, the purpose of including the flight task is to provide an increase in workload to the subjects.
This is in marked contrast to our previous experiments, which had subjects perform button pressing as the sole task.

Subjects were divided into two groups based on which simulator setup they used: Touchscreen (TS) or Virtual Reality (VR).
The TS group would use a touchscreen to input the text on the instruments, while the VR group used the R3C system (passive haptics, hand tracking and virtual reality).
In this section, the hardware used and instrument designs that were developed are described.
This is followed by a description of the task that the subjects performed.

\subsection{Simulator Setup}

The simulator workstation as configured for each group is shown and annotated in Figure~\ref{fig:de_simgroups}.
Figure~\ref{fig:de_userpic} shows a user in each group operating the simulator.
The joystick is mounted on the left side of the desk, and subjects used their left hand to operate it.
The instrument is mounted in front of the subject, at a height and position similar to an instrument in a cockpit.
The buttons on the instrument were operated with the right hand of the subject.
The hand tracker is mounted above the instrument looking down, a similar setup to the previous experiments.
Both groups had an aural indication (a click noise of a button being pressed) when a button was pressed on the instrument, using speakers mounted behind the instrument panel.
The joystick and instrument were positioned in the same location for each group.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{de_vr_photo.png}
        \caption{Virtual Reality (VR) Group.}
        \label{fig:de_simgroups:vr}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{de_ts_photo.png}
        \caption{Touchscreen (TS) Group.}
        \label{fig:de_simgroups:ts}
    \end{subfigure}
    \caption{Simulator configured for each group.}
    \label{fig:de_simgroups}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{de_vr_user.jpg}
        \caption{Virtual Reality (VR) Group.}
        \label{fig:de_userpic:vr}
    \end{subfigure}
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{de_ts_user.jpg}
        \caption{Touchscreen (TS) Group.}
        \label{fig:de_userpic:ts}
    \end{subfigure}
    \caption{Simulator being used in each group configuration.}
    \label{fig:de_userpic}
\end{figure}

The major physical difference between the two groups was the instrument.
For the Virtual Reality (VR) group, 3D printed instruments were used to provide the passive haptics, while the input was read by the hand tracker.
The Touchscreen (TS) group used a touchscreen which displayed the instrument buttons, mounted at the same location.

The VR group used the hand tracker activated system previously described in Chapter \ref{chap:prototype}.
For this experiment, the buttons were configured to turn a blue color when the hand tracker registered a finger within the zone.
The zones were extended \SI{0.1}{\inch} around the border of the button, raised a height of \SI{0.5}{\inch} above the surface of the button.
When the button was activated after the 150~millisecond delay, the blue highlight would disappear and the button in the virtual world would move inwards as if it were being pushed in\footnote{Of course, the physical button could not and did not move.}, the press sound would play, and the response of the instrument associated with pressing that button would occur.
A separate release sound would play when the finger left the zone after a successful press, and for the VR group the button would move back to its starting position.

The TS group used a \SI{10.1}{\inch} capacitive touch screen with resolution of 1024x600.
The active area of the screen was \SI{8.8}{\inch} by \SI{5.1}{\inch}, with outside dimensions of \SI{10.4}{\inch} by \SI{6.7}{\inch}.
The instrument buttons were rendered in a web browser using standard HTML elements.
Javascript press and release events were used to simulate the same behavior as described for the VR group, except for the highlighting before a button press.

%It was designed to be as similar as possible between the two configurations.
%Neither group had ``out the window'' visuals, and relied only on the attitude indicator on the instrument to judge the attitude of the plane.
%For the Virtual Reality (VR) group, the visuals showed a plain interior of a cockpit, but the out-the-window view was black.
%The visuals of the tracker were rendered on top of the browser window with the same OpenGL rendering code used for the VR group.
\subsection{Instrument Designs}

The two different designs used were developed to be both realistic as a cockpit instrument design that would be under consideration, yet still have one design with flaws that should be identified during a design evaluation.
There were three major components of each design: the buttons for text input, a text display area, and an attitude indicator for the flight task.
The major difference between our two designs is the location of the buttons and the logic of the text entry task.
The `Keypad' design (Figure~\ref{fig:de_vr_instrument:keypad}) has the button keys on the right side and the attitude indicator on the left.
The `Edgekey' design (Figure~\ref{fig:de_vr_instrument:edgekey}) has the button keys split on either side of the attitude indicator.
Both designs placed the text display area underneath the attitude indicator.
The logic of the text entry task will be described in more detail in the next section, but to understand the two designs it is only necessary to know that the entries were limited to the letters `A' through `F' and the numbers `1' through `6'.

%The tracking task display was the same size on the display for both designs.
%The prompting task text was placed below the tracking task display, and the same font, size and color was used for both designs.
%The prompting task text font was approximately \SI{0.62}{\inch} tall.
%These were kept consistent to limit the number of possible variables between the two designs.
%The prominent difference is the placement and behavior of the buttons which is described in this section.

The two designs are pictured in three different versions developed for the experiment.
Figure~\ref{fig:de_vr_instrument} shows the instruments as they were rendered in the virtual world for the VR group.
The 3D printed versions that provided the passive haptics for the VR group are photographed in Figure~\ref{fig:de_3d_instrument}.
The TS group used a fullscreen web browser to render the instrument elements on the touchscreen.
Screenshots of each design are shown in Figure~\ref{fig:de_ts_instrument}.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{keypad_vr.png}
        \caption{Keypad Design}
        \label{fig:de_vr_instrument:keypad}
    \end{subfigure}
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{edgekey_vr.png}
        \caption{Edgekey Design}
        \label{fig:de_vr_instrument:edgekey}
    \end{subfigure}
    \caption{The instrument designs as shown in the VR group virtual environment.}
    \label{fig:de_vr_instrument}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{keypad_3d.jpg}
        \caption{Keypad Design}
        \label{fig:de_3d_instrument:keypad}
    \end{subfigure}
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{edgekey_3d.jpg}
        \caption{Edgekey Design}
        \label{fig:de_3d_instrument:edgekey}
    \end{subfigure}
    \caption{The 3D printed versions of the instrument designs.}
    \label{fig:de_3d_instrument}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{keypad_ts.png}
        \caption{Keypad Design}
        \label{fig:de_ts_instrument:keypad}
    \end{subfigure}
    \begin{subfigure}[t]{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth,height=0.45\textheight,keepaspectratio]{edgekey_ts.png}
        \caption{Edgekey Design}
        \label{fig:de_ts_instrument:edgekey}
    \end{subfigure}
    \caption{The instrument designs as shown to the TS group on the touchscreen.}
    \label{fig:de_ts_instrument}
\end{figure}

The Keypad design included a button for each character that needed to be entered.
The buttons are \SI{1}{\inch} by \SI{0.75}{\inch}, with about \SI{0.26}{\inch} between buttons horizontally and \SI{0.38}{\inch} vertically.
Each button has the label directly on the top of the button.
The 3D printed instrument used for the VR group had the buttons raised a height of \SI{0.31}{\inch} from the surface of the instrument.
The button labels were also raised to provide a tactile feedback.
The font was approximately \SI{0.36}{\inch} tall, and the labels were embossed above the button surface by \SI{0.05}{\inch}.

In the Edgekey design, there is not a single button for every number and letter.
Instead, the bottom button on either side would switch the behavior (and labels) of the remaining six buttons from being 1 through 6 to A through F.
In other words, the bottom ``switching'' buttons would change the rest of the buttons from the numbers to the letters, and vice-versa.
The labels were placed offset from the button on the ``screen'' portion of the instrument, allowing them to change dynamically.
The fonts were approximately \SI{0.32}{\inch} tall, and were blue on the screen.
The buttons are smaller in this design, at \SI{0.76}{\inch} by \SI{0.55}{\inch}.
A smaller button size was needed to fit the labels and the buttons side by side.
The spacing between buttons vertically is the same as the Keypad design at \SI{0.38}{\inch}.
The center to center distance between the two sides of the button rows is \SI{7.3}{\inch}.
The 3D printed instrument version had raised nubs on each button covering half the width, \SI{0.08}{\inch} tall and raised \SI{0.05}{\inch}.
As with the Keypad design, the buttons had the same height of \SI{0.31}{\inch} from the surface.
%The difference in logic will be discussed in more detail with the description of the tasks in Section~\ref{sec:de_prompting_task}.

\subsection{Task Design}



%\begin{itemize}
%    \item Flight task using a standard joystick
%    \item Additional task that requires use of multiple buttons on the instrument
%    \item Able to develop simulator for both touchscreen and R3C setup %\tinytodo{could explain what this limits}
%    \item Able to design two different layouts with one design having distinct flaws
%    \item Simple design, yet complex enough task to have sufficient workload
%    \item Operationally relevant tasks analogous to those required in a cockpit
%\end{itemize}
%
%Ultimately, we designed a task that required number and letter inputs using the buttons, while simultaneously flying a pitch disturbance profile.

The subjects were performing two tasks simultaneously: a flight task with their left hand and a text entry task with their right hand.
The text entry task required subjects to read and memorize a short string of characters and enter it back using the buttons on the instrument.
We call the text they read and enter the `prompt', and thus the text entry task is called the `prompting task'.
For the flight task, the subjects were asked to fly in pitch only using an attitude indicator.
Their task was to keep the pitch at zero (level flight) while disturbances were introduced.
This is a standard compensatory tracking task.
The flight task is called the `tracking task'.
These two tasks are described in detail in this section.

\subsubsection{Prompting Task}

The prompting task was designed to be both a realistic task for a cockpit as well as a demanding task when done concurrently with the tracking task.
The task required the subjects to read and memorize a short string of characters and enter it back using the buttons on the instrument.
To limit the task physically (by number of buttons) and mentally, the characters used were the numbers 1 through 6 and the letters A through F.
The prompts were 4 characters long.
Once the subject started entry, the prompt would disappear, forcing them to hold it in working memory.
It has been well established that humans are capable of holding about seven digits in working memory \citep{miller_magical_1956,baddeley_working_1992}.

The sequence of the prompts was separated into 10~second ``windows''.
The prompt would appear randomly between 2 and 3~seconds of the start of the window.
From the time of appearance, subjects were given seven seconds until timeout.
When the subject pressed the first button of the prompt, the prompt itself was cleared and asterisk symbols ($*$) were shown in place of the prompt for each button entry by the subject.
If the subject ran out of time, the text in entry area would return to black.
Although subjects were briefed on the timeout and given practice to learn the pace, no warning or indication of time left was shown during the trials.
Whether they completed the prompt within the time limit, or they timed-out, this process was repeated every 10~seconds.
%This meant that subjects had at least 3 seconds of time with no prompt.

The prompts themselves were always composed of three numbers followed by a letter or three letters followed by a number.
This structure was decided upon to provide a consistent pattern, yet still utilize both letters and numbers in every prompt.
The prompts were randomly chosen but were not allowed to have repeat numbers or letters. %, and for the prompts with three letters, common words or acronyms were filtered out (e.g.\ ``BAD'', ``FDA'').
The selection of letters or numbers as the first three characters was randomly chosen as well, with an equal weight to each.

The Edgekey design had fewer buttons, but the logic of the buttons generated higher workload demands.
While some of the more subtle differences were expected to be noted by the evaluation study (e.g.\ having smaller buttons, different position of the flight task), the major flaw designed into the Edgekey design was the requirement that the subject uses the switching key to change from letters to numbers and back.
This additional action fundamentally changed the demands of the prompting task, as the subjects now had to press this additional button to change labels at least once per prompt.
The cognitive workflow of the subject is diagrammed in Figure~\ref{fig:de_flowchart}.
The additional mental effort of the Edgekey design is shown in the dashed box, where the subject has to verify the state of the instrument and possibly press the switch button before they press the buttons of the prompt.
Since the prompts were kept as a consistent format of three of the same type and fourth of the other, this extra work was easily skipped for most buttons, and anticipated between the third and fourth button.
There was no guarantee that the next prompt would start with the instrument in the correct state for the new prompt, so there was always an additional cognitive load in determining whether a switch was necessary at the beginning of the prompting window, which would be accompanied with the physical effort if the switch was needed.

\begin{figure}
    \centering
    \tikzstyle{decision} = [diamond, draw, fill=blue!20, text width=7em, text badly centered, inner sep=0pt]
    \tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=8em, text centered, rounded corners, minimum height=2em]
    \tikzstyle{line} = [draw, -latex']
    \tikzstyle{cloud} = [draw, ellipse,fill=red!20, minimum height=2em]
    \begin{tikzpicture}[auto]
        \node [block] (read) {Read Prompt};
        \node [block, below=of read] (memorize) {Memorize Prompt};
        \node [block, below=of memorize] (recall) {Recall $n^{\mathrm{th}}$ Character};

        \node [decision, below=of recall] (design) {Design?};
        %\node [cloud, below=of recall] (Keypad) {Keypad};

        \node [draw=none, text width=7em, below=2cm of design] (targetjoin) {};
        \node [block, below=1cm of targetjoin] (target) {Find \& Target $n^{\mathrm{th}}$ Button};
        \node [block, below=of target] (press) {Press $n^{\mathrm{th}}$ Button};
        %\node [draw=none, left=of target] (nloop1) {};
        \node [draw=none, below=0.5cm of design] (keypadlabel) {Keypad};
        \node [draw=none, left=of keypadlabel, text width=6em, text centered] (nloop) {Repeat for 4 Characters};

        \node [decision, right=1.5cm of targetjoin] (decide) {Are labels for $n^{\mathrm{th}}$ character shown?};
        %\node [cloud] (Edgekey) at (design -| decide) {Edgekey};
        \node [block, below=of decide] (targetswitch) {Find \& Target Switching Button};
        \node [block, below=of targetswitch] (pressswitch) {Press Switching Button};
        \node [block, below=of pressswitch] (recall2) {Recall $n^{\mathrm{th}}$ Character};

        \path [line] (read) -- (memorize);
        \path [line] (memorize) -- (recall);
        \path [line] (recall) -- (design);
        %\path [line] (design) -- node [fill=white, near start, center] {Keypad} (target);
        \path [line,-] (design) -- (keypadlabel);
        \path [line] (keypadlabel) -- (target);
        \path [line] (target) -- (press);
        \path [line,-] (press) -| (nloop);
        \path [line] (nloop) |- (recall);

        %\path [line, dashed] (recall.south) ++(0,-0.5cm) -| (Edgekey);
        \path [line] (design) -| node [near start, above] {Edgekey} (decide);
        \path [line] (decide) -- node [above] {Yes} (targetjoin.center);
        \path [line] (decide) -- node {No} (targetswitch);
        \path [line] (targetswitch) -- (pressswitch);
        \path [line] (pressswitch) -- (recall2);
        \path [line] (recall2.east) -- ++(1.0cm,0) |- (decide.east);

        \draw[thick, dashed] ($(design.east)+(0.8,1.0)$) rectangle ($(recall2.south east)+(1.6,-0.6)$);
        %\draw[thick, dashed] ($(decide.north west)+(-1.8,3.3)$) rectangle ($(recall2.south east)+(1.3,-0.6)$);

    \end{tikzpicture}
    \medskip
    \caption{Prompting Task Flowchart of Cognitive Work for Each Design. Extra work of Edgekey design enclosed in dashed line box.}
    \label{fig:de_flowchart}
\end{figure}

\subsubsection{Tracking Task}
\label{sec:de_fdm}

The tracking task display was a standard attitude indicator display, as can be seen in Figure~\ref{fig:de_vr_instrument}.
Each tick corresponds to 1~degree in the dynamics simulation, with major ticks at intervals of 5~degrees.
The attitude indicator was rendered to the size of \SI{3.4}{\inch} square on the instrument.
Subjects controlled the one-dimensional (pitch only) task using a joystick with their left hand.
The joystick is pictured in Figure~\ref{fig:de_simgroups}.

%\begin{figure}
%    \centering
%    \includegraphics[width=3.4in]{ai_display.png}
%    \caption{Attitude Indicator Display}
%    \label{fig:de_ai_display}
%\end{figure}

The block diagram of the dynamics is shown in Figure~\ref{fig:de_block_diagram}.
The dynamics model was updated and recorded at a rate of \SI{125}{\hertz}.
The output of the joystick, $r_{js}$, varies from $-1.0$ to $1.0$, and the gain of \ang{10} was chosen to ensure the pilot had enough control authority to complete the task.
The flight dynamics model (Aircraft Dynamics) of the simulator was a stability derivative based model for a Boeing~747 from NASA CR-2144 \citep{heffley_aircraft_1972}, listed as ``Flight Condition 2''.
The model was linearized from sea-level flight at an airspeed of 335~ft/s.
The configuration of the airplane had the gear up, no flaps, total weight of \num{654000}~lbs, and an angle of attack of \ang{7.3}.
This dynamics model was chosen due to availability and the specifics of the dynamical model were not important other than providing a response similar to an aircraft in flight.
The transfer function of the aircraft dynamics is given as:

\begin{figure}
    \centering
    \centering
    \tikzstyle{block} = [draw, rectangle, minimum height=3em, minimum width=6em]
    \tikzstyle{smallblock} = [draw, rectangle, minimum height=3em, minimum width=3em]
    \tikzstyle{sum} = [draw, circle, node distance=1cm]
    \tikzstyle{input} = [coordinate]
    \tikzstyle{output} = [coordinate]
    \begin{tikzpicture}[auto, node distance=2cm,>=latex']
        \ssp
        \node[input, name=input] {};
        \node[smallblock, right=of input] (K) {$10$};
        \node[sum, right=of K] (sum) {};
        \node[block, node distance=1.5cm, above=of sum] (disturbance) {Disturbance};
        \node[block, right=of sum, text width=5em, align=center] (aircraft) {Aircraft Dynamics};
        \node [output, right=of aircraft] (output) {};

        \draw [draw,->] (input) -- node {$r_{js}$} (K);
        \draw [->] (K) -- (sum);
        \draw [->] (disturbance) -- node {$\theta_D$} (sum);
        \draw [->] (sum) -- node {$\theta_{el}$} (aircraft);
        \draw [->] (aircraft) -- node {$\theta$} (output);
    \end{tikzpicture}
    \medskip
    \caption{Tracking Task Dynamics Block Diagram. $r_{js}$ is the joystick input. $\theta_{el}$ is the elevator input. $\theta_D$ is the disturbance. $\theta$ is the pitch of the aircraft.}
    \label{fig:de_block_diagram}
\end{figure}

\begin{gather}
        \frac{\theta}{\theta_{el}}
        =
        \frac{
            -0.572 (s+0.553) (s + 0.0396)
        }{
            (s^2 + 2\zeta_1 \omega_1 + {\omega_1}^2)
            (s^2 + 2\zeta_2 \omega_2 + {\omega_2}^2)
        }\\
    \omega_1 = 0.0578, \quad \zeta_1 = 0.0160 \nonumber \\
    \omega_2 = 1.12, \quad \zeta_2 = 0.798 \nonumber
\end{gather}

The disturbance model is from \citet{sweet_identification_1999}.
It is designed to provide a broad spectrum of frequencies for human pilot frequency response identifications \citep{mcruer_mathematical_1974}.
The disturbance is a sum of sines described by:
\begin{align}
    \theta_D = K\sum\limits_{i=1}^{12} \left[ a_i \left(\frac{2\pi k_i}{240} \right) \sin\left( \frac{2\pi k_i}{240}t + \phi_i \right) \right]
\end{align}
The $k_i$ terms are given as:
\begin{align*}
    k_1 &= 7,   & k_2 &= 11,  & k_3 &= 16,  & k_4    &= 25,  & k_5    &= 38,  & k_6    &= 61 \\
    k_7 &= 103, & k_8 &= 131, & k_9 &= 151, & k_{10} &= 181, & k_{11} &= 313, & k_{12} &= 523
\end{align*}
The amplitude terms are $a_i=0.5$ for $i <= 6$ and $a_i=0.05$ otherwise.
The phase terms, $\phi_i$, were randomly selected on the $(-\pi, \pi)$ interval ensuring a uniform distribution \citep{sweet_identification_1999}.
This random selection was precalculated for each trial, however the order was repeated for each subject so there was no between subject variance in the disturbance signal.
Furthermore, each subject received the same sequence of disturbance signals for each instrument design.
The disturbance amplitude, $K$, was chosen such that the root-mean square (RMS) of the signal was $3.5$~degrees.
The value of the RMS was chosen through pilot studies to ensure the task was challenging but not overwhelming.

\subsection{Experiment Design}

Subjects were divided into the two groups, Touchscreen (TS) and Virtual Reality (VR).
The overall sequence of the experiment started with a training session on the simulator and the task, followed by an evaluation session for each of the two designs, finishing with questionnaires asking subjects to evaluate the two designs.
The timeline of the experiment was the same for each subject, except for counterbalancing the order that the designs were evaluated.
The training portion started with a slide deck explaining the tasks, the simulator that the subject was using (depending on which group they were in), and the functionality of the two designs.
Next, they performed practice trials with just the tracking task and then just the prompting task.
The practice trials of the tracking task were 60~seconds long and repeated until the subjects' performance had flat-lined.
This took between three to six trials for each subject.
%After basic familiarity with the technology

For the evaluation sessions with each design, they performed six trials with both tasks.
The first three were a minute long, and were considered practice trials, and not included in the data analysis, though this was not communicated to the subjects.
The following three trials were two minutes each, and were used for the analysis.
Each evaluation session concluded with a two minute trial of just the tracking task without the prompting task.
This was included to investigate if the subject had improved or fatigued at the tracking task throughout the experiment.

Subjects spent between one to two hours in the experiment (typically longer for the VR group due to additional setup).
The training and setup took approximately fifteen to thirty minutes, and each of the two evaluation sessions took about fifteen to twenty minutes to complete.
The remainder of the time was spent on the subjects completing the questionnaires.

The independent variables of the experiment are Group and Design.
The Group is the simulator the subject used, a between subjects factor, and either TS or VR.
The Design is a within subjects factor, the two instrument designs that every subject evaluated --- Edgekey and Keypad.

\subsection{Dependent Measures}
\label{sec:de_dependent}

The dependent measures were chosen to evaluate the performance of each task individually as well as the workload of the subject.
For the tracking task, the root-mean square error (RMSE) was calculated for each trial \citep{harris_human_2011}.
The subjects performed a compensatory tracking task, attempting to keep the attitude indicator at zero degrees while disturbances were introduced.
This means that the error is the pitch shown on the attitude indicator, since the goal at all times was zero pitch.
%The error in this case is simply the pitch shown to the subject, the output of the flight model described in Section \ref{sec:de_fdm}.

The prompting task has two dependent measures, for speed and accuracy.
For speed we consider the \textit{response time}, defined as the time between when the prompt is first shown to the subject and when they press the first button of their response entry.
The accuracy is measured by how many prompts they complete correctly.
Twelve prompts are shown to the subject within each trial.
The response time did not include trials which used the switch button of the Edgekey design, or trials where the first button pressed was incorrect.
The response time was averaged per trial first and then per design for each subject, and the number of correct prompts is averaged per design for each subject.

A NASA Task Load Index (TLX) survey \citep{hart_development_1988} was administered after they completed each design to measure the workload of the subject.
The TLX survey records a rating of workload between 0-100 for the following subscales: Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, and Frustration.
Our implementation allowed selection of the ratings within increments of 5, and included anchors of ``Low'' and ``High'' at the extrema of 0 and 100, respectively (except for Performance, which uses ``Good'' and ``Bad'').
The midpoint was also visually indicated with a larger tick.
The ranked pairs modification was used and completed for both times the subject took the survey.
This modification asks the subject, for each of the pairwise combinations of subscales, which they felt contributed more to their workload.
The number of times they select each subscale is used a weight to calculate a weighted mean for the total TLX score.

Finally, the subjects were given a questionnaire asking for their feedback on each instrument design.
For each design, the subjects were asked the following questions:
\begin{itemize}
    \item Please comment on any difficulties you had performing the prompting task with this design especially in contrast to the other design.
    \item Please comment on anything you liked in this design.
    \item Please comment on anything you did not like in this design.
    \item Any other comments?
\end{itemize}
Additionally, the following questions were asked:
\begin{itemize}
    \item Which instrument design did you prefer? Why?
    \item Did you experience any physical fatigue during the experiment? Where?
    \item Any other comments?
\end{itemize}
An open form text box was used for the response field for each of these questions.

In a standard design evaluation study, the feedback received from the users in this questionnaire (and other debriefing interviews) would often be the main source for carrying out re-design.
%awk
The purpose of this feedback in this experiment is to determine and document in which ways does this feedback differ between groups.
For example, if most subjects in one group noted issues with the size of a button, while no one in the other group found an issue with that button, this would indicate that using this VR system may not highlight the same issues regarding button sizes.
The groups were purposely left ambiguous in the example, as it does not matter which group found the flaw and which group did not comment on it.
Although we could postulate as to which group are ``correct'' in their evaluation of the instrument, it is not a useful exercise, as the only result is to document what potential differences could arise so that users of this system can be aware.

With that goal in mind, the analysis of the feedback questions seeks to identify differences between the groups.
The sentences from the open form responses were first separated into single feedback comments, and summarized using common language.
If a single subject repeated the same comment in the answers to multiple questions, they were only counted once.
Each of these simplified feedback comments were assigned to a category or overall summary of their feedback.
This process was completed separately for each group.
We aim to look for feedback that is unique to a certain group or feedback that receives a higher frequency of comments in one group.
This will provide a summary of where the groups provide the same feedback and where they provide differing feedback.

\subsection{Hypotheses}
\label{sec:de_hypotheses}

The main hypothesis of this experiment is that the use of a VR/R3C simulator will not affect the conclusions of a design evaluation study, compared to a traditional touchscreen simulator.
We do expect that some of the dependent measures may have a significant difference in Group (one Group will perform a task better overall, no matter the Design) or a significant difference in Design (subjects will perform better with one Design, no matter the Group).
The more important measure for us, however, is the interaction effect.
This will test if the change between Designs is similar for the two Groups.
If this is the case, then it may indicate that an evaluation study using one of these simulators could draw differing conclusions of an evaluation study using the other.
Statistically, we will test the hypothesis that there exists no interaction effect between Group and Design for any of our dependent measures.

Additionally, the two tracking only trials performed at the end of each evaluation session, as well as the final tracking only training trial, will be used to investigate if the subjects were still learning the tracking task.
The concern if subjects became more trained in the tracking task is that it could lower their amount of attention to that portion of the task, causing a change in performance on the prompting task that was not due to the design change.
These hypotheses are enumerated here:

\begin{enumerate}[label={H\arabic*.}]
    \item The tracking task RMSE will have no interaction effect between Group and Design
    \item The prompt response time will have no interaction effect between Group and Design
    \item The percent of correct prompts will have no interaction effect between Group and Design
    \item The NASA-TLX scores will have no interaction effect between Group and Design
    \item The tracking task RMSE for the last training trial and the tracking only trials will not change throughout the experiment
\end{enumerate}

\subsection{Statistical Tests}

The quantitative dependent measures are tested with a two-way ANOVA, with one within subjects factor (Design) and one between subjects factor (Group).
The Design factor contains two levels, the two designs each subject tested, Edgekey and Keypad.
The Group factor also contains two levels, the VR (Virtual Reality) group and the TS (Touchscreen) group.
When the ANOVA showed significance in the interaction test, post-hoc repeated measured t-tests were undertaken to determine the significance of Design within each Group.
Independent samples t-tests were used to test the significance of Group within each Design.
The last hypothesis testing the effects of learning on the trials with only the tracking task will be tested with a two-way ANOVA, with the Group as a between subjects factor, and the trial number as a within subjects factor.
The trial number is chronological in the order the subjects performed them.
The first trial was the last tracking only training trial, and the next two were tracking only trials at the end of each design evaluation.

Statistical significance level was corrected using the Bonferroni correction considering the 5 hypotheses being tested.
All effects were considered statistically significant at the 0.01 level ($\alpha = 0.05/5 = 0.01$).
Effects which have a significance level between $0.05<p<0.01$ are considered to be marginally significant.

\section{Results}

\subsection{Demographics}

Twenty-three subjects were recruited from the UC Davis engineering undergraduate and graduate student population.
Twelve subjects were placed in the VR group, and the remaining eleven in the TS group.
The mean age was 21.0 ($\sigma = 3.14)$, with 19 male and 4 female subjects.
The genders were balanced between the two groups.
Two of the subjects were left-handed, with one in each group.
Most subjects had no flight experience (two were student pilots), and all of the VR group subjects indicated that they had less than one hour of experience using virtual reality headsets.
It should be noted that the subjects are not the beneficial population of the research.
The task and experiment was designed with this in mind and mitigated through training and the simplicity of the task design.

\subsection{Performance Measures}

\subsubsection{Tracking Task RMSE}

\begin{figure}
    \centering
    \includegraphics[width=5.0in]{{rmse.3.0x1.5}.png}
    \caption{RMSE (root mean square error) of the tracking task by Group and Design. Error bars are standard error of the mean.}
    \label{fig:de_factor_rmse}
\end{figure}

The performance of subjects performing the tracking task was measured using the root-mean square error (RMSE) of the pitch.
The RMSE of each Group and Design is shown in Figure~\ref{fig:de_factor_rmse}.
The effect of Group yielded an $F$ ratio of $F(1, 21) = 21.4, p < 0.001$ indicating a significant difference between VR ($M=\ang{1.28}, \sigma=\ang{0.38}$) and TS ($M=\ang{1.97}, \sigma=\ang{0.38}$).
In both groups, subjects were performing the tracking task using their left hand to control the same joystick.
There are two potential factors that could contribute to the decreased performance in the tracking task for the VR group.
The most direct factor is the loss of visual acuity in the tracking task display (attitude indicator) due to the resolution of the VR head-mounted display.
Indirectly, the additional workload of the prompting task for the VR group could be taking attention away from the tracking task.
The effect of Design indicated a marginally significant difference ($F(1, 21) = 5.94, p=0.024$) for the tracking task RMSE between Keypad ($M=\ang{1.57}, \sigma=\ang{0.51}$) and Edgekey ($M=\ang{1.70}, \sigma=\ang{0.52}$).
The only change in the tracking task display between the two instrument designs is a small change in position.
It moves from being on the left side for the Keypad to the middle for the Edgekey.
Since there was no change otherwise, this suggests that any difference on the tracking task performance between the designs would be related to additional workload from the prompting task.

The main hypothesis (H1.\ The tracking task will have no interaction effect between Group and Design) was supported by the statistical test.
The interaction effect was not significant ($F(1, 21) = 0.17, p=0.69$).
This indicates that even though the VR group had a higher RMSE overall, the difference between designs was consistent between the groups.
An evaluation session performed with only the VR system would find the same difference between designs with the RMSE measure as a traditional simulator.

If the visual acuity was the only factor affecting performance between groups, we would expect to see a similar effect for the trials where the subjects were only performing the tracking task.
The subjects ran a single trial that was just the tracking task without the prompting task at the end of each evaluation session.
The RMSE results for these tracking only trials are shown in Figure~\ref{fig:de_factor_rmse_tracking_only}
The effect of group on RMSE for the tracking-only trials yielded a marginally significant difference ($F(1, 21) = 4.81, p = 0.039$) between the VR Group ($M=\ang{1.32}, \sigma=\ang{0.50}$) and the TS Group ($M=\ang{0.91}, \sigma=\ang{0.43}$).
There was no significant difference for the effect of Design ($F(1,21) = 0.068, p=0.80$).
The interaction effect between Group and Design was also not significant ($F(1,21) = 3.21, p=0.087$).
This indicates that when the subjects were focused on the single task, they were able to mitigate most of the visual resolution differences between using a touchscreen and the virtual reality screen.

\begin{figure}
    \centering
    \includegraphics[width=5.0in]{{rmse_tracking_trials.3.0x1.5}.png}
    \caption{RMSE of tracking task for tracking only trials by Group and Design. Error bars are standard error of the mean.}
    \label{fig:de_factor_rmse_tracking_only}
\end{figure}

Although the tracking only trials found a marginally significant difference for the group, the difference was much more distinct for the trials with both tasks.
Additionally, the marginally significant difference between the designs for the trials with both tasks was reduced to no significance when the additional prompting task was removed.
This also points to the additional workload of the prompting task causing a performance drop on the tracking task.
The factors leading to the added workload of the prompting task are investigated in the next performance measures discussed.

\subsubsection{Prompt Response Time}

The first measure of the prompting task is the response time of the subject.
The response time is defined as the time from the prompt is shown to each subject until they press the first button of the prompt.
The response time of a prompt was not included if the subject did not press the correct letter or number for the first button (6.5\% of prompts).
However, the correctness of their response after the first button was not considered.
For the Edgekey design, it would be possible that the subject had to start with the switching button if the new prompt did not start with the same mode (letters or numbers) as the previous prompt (see Figure~\ref{fig:de_flowchart}).
Since this button would not clear the prompt when it was pressed, it is not considered the first button of their entry.
However, this would still require an additional movement of the subject, adding additional time.
For this reason, the prompts which required the subject to start with the switch key are filtered out of this analysis.
After filtering, 885 of the total 1700 prompts recorded for the Edgekey design were kept.

\begin{figure}
    \centering
    \includegraphics[width=5.0in]{{reaction_time.3.0x1.5}.png}
    \caption{Prompt Response Time (milliseconds) by Group and Design. Error bars are standard error of the mean.}
    \label{fig:de_factor_response}
\end{figure}

The response time by Group and Design is shown in Figure~\ref{fig:de_factor_response}.
The response time was unique among the dependent measures, as all tests were insignificant.
The effect of group yielded an $F$ ratio of $F(1, 21) = 1.19, p = 0.29$ indicating no significant difference between VR ($M = \SI{2812}{\milli\second}, \sigma = \SI{383}{\milli\second}$) and TS ($M = \SI{2594}{\milli\second}, \sigma = \SI{567}{\milli\second}$).
One factor that could influence the response time between groups is the additional time to activate a button in the VR environment versus the touchscreen.
The touchscreen subjects were using a familiar interface for activating the buttons, while the VR subjects needed to activate the button with the virtual hand.
We previously saw the difference in performance between targeting buttons in the real world and virtual world in the first experiment (Chapter~\ref{chap:pointing}).
However, a large portion of the response time for the subject is their cognitive processing of the prompt -- recognizing the new prompt has appeared, reading it, then memorizing it.
Beyond potential differences in the visual environment, the cognitive portion should not take more time for one group or the other.
A potential reason that there could be a lower than expected difference between the group means is that it was observed during the experiment that some VR subjects learned to keep their hand extended closer to the instrument so that the hand tracker could keep it in view.
When the hand tracker lost view of the hand, the re-acquisition time could be significant, so holding it close to the instrument would prevent this from happening.
This issue comes up again when looking at the subjects' response to questions about fatigue.

The effect of design was also insignificant ($F(1, 21) = 0.68, p = 0.42$) between Keypad ($M=\SI{2728}{\milli\second}, \sigma=\SI{512}{\milli\second}$) and Edgekey ($M=\SI{2687}{\milli\second}, \sigma=\SI{471}{\milli\second}$).
The biggest difference between the two designs is the switching key on the Edgekey design.
As described above, the need for an additional switch press before the first prompt button was filtered out, so we are only comparing prompts where the first button was available right away to the subject.
Even though the physical requirements were filtered out, subjects still need to verify that the labels are in the correct state for starting entry.
Since the Edgekey design had more time pressure due to the need for the switch key, subjects could have learned to respond quicker to adapt for this.
However, these differences in the design did not appear to have a significant effect on the response time.

Finally, the interaction effect was not significant ($F(1, 21) = 0.001, p = 0.96$), supporting the hypothesis (H2.\ The prompt response time will have no interaction effect between Group and Design).
Unlike with the RMSE, however, we did not see a difference between groups or designs.
This indicates that for a task that includes a large cognitive workload, the performance measures may provide the same result with a VR system than would be expected with a traditional simulator.

\subsubsection{Prompt Completion Accuracy}

\begin{figure}
    \centering
    \includegraphics[width=5.0in]{{correct_prompts.3.0x1.5}.png}
    \caption{Percent of Correct Prompts per Trial by Group and Design. Error bars are standard error of the mean.}
    \label{fig:de_factor_correct}
\end{figure}

The second measure of the prompting task is the accuracy of the subjects in correctly completing the prompt.
The prompt task consists of two sequential components: memory and execution.
First, they must remember the prompt as they enter it, and second, they must be able to physically press the buttons within the seven second response window.
For the statistical test, we are using the percent of how many prompts each subject completed successfully per trial.
Among the incorrect prompts, we can differentiate between whether the subject entered the prompt incorrectly (failure to remember the prompt) or whether the subject ran out of time (failure to physically press the buttons).
These are reported to help analyze the results, but are not used in the statistical tests.
There were 12 prompts per trial, and every subject completed three trials for each design.

The percent of correct prompts are reported on here, shown for each Group and Design in Figure~\ref{fig:de_factor_correct}.
The percent of correct prompts had a significant interaction effect between group and design ($F(1, 21) = 27.8, p < 0.001$), meaning the main effects must be interpreted with the post-hoc tests as well.
This is the first dependent measure that rejects the null hypothesis (H3.\ The percent of correct prompts will have no interaction effect between Group and Design).
Both main effects were significant, the effect of group yielded an $F$ ratio of $F(1, 21) = 43.9, p < 0.001$ while the effect of design yielded an $F$ ratio of $F(1, 21) = 64.1, p < 0.001$.

For the effect of design on the VR group, the repeated measured t-test indicated a significant difference ($t(11) = 8.0, p < 0.001$) between the Keypad ($M = 67.6\%, \sigma = 13.47\%$) and the Edgekey ($M = 33.3\%, \sigma = 19.8\%$)
The TS group had a marginally significant difference ($t(10) = 2.28, p = 0.045$) between Keypad ($M = 88.4\%, \sigma = 8.0\%$) and the Edgekey ($M = 81.6\%, \sigma = 11.7\%$).
These results indicate that both groups had trouble with the additional time pressure caused by the Edgekey design requiring the use of the switch key.
The TS group performed closer to their performance in the Keypad design, however, with only about 1 fewer prompts correct on average (6.8\%).
The VR group had much more difficulty in the Edgekey design, correctly completing about half as many as they completed in the Keypad design.
They also had more difficulty in both designs compared to the TS group.

\begin{figure}
    \centering
    \includegraphics[width=6.0in]{{prompts_stacked.6.0x4.0}.png}
    \caption{Average percent of correct, incorrect, and incomplete prompts by Group and Design.}
    \label{fig:de_correct_stacked}
\end{figure}

The post-hoc test for differences between groups within each design also found that for both designs the VR group had more difficulty.
These tests had significant effects for both the Keypad design ($t(21) = 4.44, p < 0.001$) between the VR group and the TS group, and the Edgekey design ($t(21) = 7.05, p < 0.001$) between the VR group and the TS group.
The main effect of group clearly has a meaningful effect, which found the VR group ($M = 50.5\%, \sigma = 24.1\%$) had significantly fewer correct prompts than the TS group ($M = 85.0\%, \sigma = 10.4\%$).
This difference is largely due to subjects not being able to complete the prompt.
Figure~\ref{fig:de_correct_stacked} shows the breakdown of the mean result of each trial for each group and design.

Across all groups and designs, between one and two prompts on average were incorrectly completed in each trial (between 9.4\% and 14.4\%).
Most of the difference in number completed correctly is due to the incomplete prompts.
A major contributing factor for this could be the method of button activation used for the VR group combined with the time pressure.
Another contribution could be the limitations of the hand tracker.
When the hand tracker lost tracking or gave bad information, it became hard or impossible for the subject to activate a button until the hand tracker returned to normal.
When this happened in the middle of a prompt, the amount of time it took to recover from the bad tracking would lead to a timeout on the prompt entry, causing an incomplete prompt.
The variance of number correct was also much larger in in the VR group, which could be caused by some subjects adapting to the unfamiliar VR environment more rapidly.

For this dependent measure, the null interaction effect hypothesis was rejected.
We expect that this was caused by the time pressure and the additional work required to activate the buttons in the VR system.
This was not completely unexpected as we have seen in previous experiments time penalty of using the hand tracker for pressing buttons in the VR system.
An evaluation study which requires time pressured button entry may not get the same results using a VR system due to this limitation.
However, in our case the same effect was found between designs within each group.
The interaction effect existed because the VR group found the Edgekey design more challenging compared to the Keypad than the TS group did.

\subsubsection{NASA-TLX}

\begin{figure}
    \centering
    \includegraphics[width=5.0in]{{tlx.3.0x1.5}.png}
    \caption{NASA-TLX workload scores by Group and Design. Error bars are standard error of the mean.}
    \label{fig:de_factor_tlx}
\end{figure}

After the subject completed their trials for each design, they filled out a NASA-TLX workload survey.
Their scores, weighted means by the pairwise comparisons, are used here as a measure of their self-reported workload.
The factor plot is shown in Figure~\ref{fig:de_factor_tlx}.
The interaction effect between group and design was found to be significant ($F(1, 21) = 8.25, p < 0.001$).
The main effects showed a significant difference in design ($F(1, 21) = 23.6, p < 0.001$), but not in group ($F(1, 21) = 1.69, p = 0.21$).
This could mean that the group did not affect the TLX score, but in the presence of an interaction effect, the post-hoc tests guide the interpretation.

The repeated measures t-tests indicated significance between designs for the VR group ($t(11) = -4.20, p = 0.001$) between the Keypad design ($M = 54.4, \sigma = 20.4$) and the Edgekey ($M = 85.6, \sigma = 11.2$)
There was a marginally significant difference between designs for the TS group ($t(10) = -2.72, p = 0.02$) between the Keypad design ($M = 61.5, \sigma = 4.46$) and the Edgekey ($M = 69.2, \sigma = 10.1$).
The effect of design was much stronger in the VR group, but both groups indicated respectively higher workload on the TLX scores for the Edgekey design.
This follows from the experimental design which predicted that the Edgekey design would be more difficult.
One factor that could have contributed to a larger difference in scores for the VR group could be the increased difficulty subjects had in completing the prompt, as seen in the results of the number of incorrect and incomplete prompts for the VR group using the Edgekey design (Figure~\ref{fig:de_correct_stacked}).
The effect of group was not shown to be significant in the ANOVA analysis, but the independent samples t-test showed a significance for the Edgekey design ($t(21) = 3.69, p < 0.01$) between the VR Group ($M = 85.6, \sigma = 11.2$) and the TS Group ($M = 69.2, \sigma = 10.1$).
With the Keypad design, The effect of group was not significant ($t(21) = -1.13, p=0.27$) between VR ($M = 54.4, \sigma = 20.4$) and TS ($M = 61.5, \sigma = 4.46$).
These tests further illustrate that the VR group found a higher workload for the Edgekey design specifically, as both groups rated the workload in the Keypad design similarly.

The subject perceived workload scores rejected the null interaction hypothesis (H4.\ The NASA-TLX scores will have no interaction effect between Group and Design).
In this case, the effects changed between the groups.
The VR group reported significantly more workload for the Edgekey design, while the TS group reported only a marginal increase.
We expect that this change was likely due to the increased work required to complete the prompt, the same reason for the change in the prompt accuracy measure which also rejected the null hypothesis.

\subsubsection{Tracking Task Learning}

Throughout the experiment the subjects performed trials with only the tracking task, instead of both the tracking task and the prompting task.
Initially, they performed a number of training trials at the beginning with only the tracking task, and then after each evaluation session there was a single trial of just the tracking task.
In this section we will test the RMSE of their final training trial and the two after-evaluation trials for any significant learning effects.
The trial number is chronological throughout the timeline of the experiment for each subject.
%This means that due to the counterbalancing, the second and third trial are done with different designs based on the subject.
Since the visual environment of the tracking task was quite different for each group, the Group factor is included as a between subjects factor.
The RMSE of the tracking only trials, by Group and trial number, is shown in Figure~\ref{fig:de_factor_training}.

\begin{figure}
    \centering
    \includegraphics[width=5.0in]{{training_rmse.3.0x1.5}.png}
    \caption{RMSE for Tracking Only Trials by Group and Trial. Error bars are standard error of the mean.}
    \label{fig:de_factor_training}
\end{figure}

The two-way ANOVA found Group to be a marginally significant factor ($F(1, 21) = 4.94, p=0.037$).
The TS Group ($M=\ang{0.94}, \sigma=\ang{0.43}$) had an RMSE \ang{0.42} lower than the VR Group ($M=\ang{1.36}, \sigma=\ang{0.51}$).
The effect of group on the tracking task was already established, so the marginal significance found here is not unexpected.
Trial number was found to have no significant effect ($F(1, 21)=3.65, p=0.069$) between the three trials.
The means of the three trials, in order, are \ang{1.23} ($\sigma=\ang{0.54}$), \ang{1.18} ($\sigma=\ang{0.51}$), and \ang{1.07} ($\sigma=\ang{0.51}$).
Even though the statistical test indicates no significance, the means do decrease as trial number increases.
This combined with the large variance suggest that some subjects were potentially experiencing some training effects, but overall the effect of training is not statistically significant.
The interaction effect of Group and trial number had no significance ($F(1, 21) = 0.16, p=0.69$).

The final hypothesis (H5.\ The tracking task RMSE for the last training trial and the tracking only trials will not change throughout the experiment) was supported by these results.
We found that the subjects did not experience significant training in the tracking task throughout the experiment.

\subsubsection{Summary}

A summary of the significance results from the ANOVA and post-hoc t-tests for all the performance measures are shown in Table \ref{tab:de_anova}.
The significance is indicated by `$*$' for $p<0.01$, `+' for $0.01<p<0.05$, and `-' for no significance.
For the measures with significant interaction effect, the post-hoc t-tests are shown per group and per design.
We found no interaction effect with the tracking task RMSE and prompting task Response Time, supporting the first two hypotheses from Section~\ref{sec:de_hypotheses} (H1 and H2).
The prompting task accuracy (Correct Prompts) and the workload (NASA-TLX) both rejected the no interaction effect hypotheses (H3 and H4).
We attribute the interaction effects found in those two dependent measures to be related to the increase in difficulty to activate buttons using the hand tracker.
The results will be discussed further (Section~\ref{sec:de_discussion}) after reporting on the results of the feedback questions.

\begin{table}
    \centering
    \includetable{de_anova_summary.tex}
    \caption{Statistical Significance Test Results. `$*$' indicates significance at the $p<0.01$ level, `+' indicates marginally significant ($0.01<p<0.05$), and `-' indicates no significance. Hypotheses are from Section~\ref{sec:de_hypotheses}, which stated that the dependent measure has no interaction effect.}
    \label{tab:de_anova}
\end{table}

\subsection{Design Feedback}

As discussed in Section \autoref{sec:de_dependent}, the long-form feedback questions were synthesized and summarized into categories.
The categories and the counts of comment occurrence for each group is summarized in Table~\ref{tab:de_feedback_sorted}.
Categories which only received one comment are not included in this table in interest of brevity, but the full table is shown in Appendix \ref{tab:de_feedback_full}.

\begin{table}
    \centering
    \includetable{de_feedback_sorted.tex}
    \caption{Counts of Design Feedback Comments per Group. Sorted by sum of comments.}
    \label{tab:de_feedback_sorted}
\end{table}

By far the issue that received the most feedback was the difficulty of using the switch key (Edgekey, Switch Difficult).
Most of the complaints stated the extra difficultly of having to press another button.
Some of the other complaints from this category were: it took extra time (with no extra time given), it added to the mental demands of the task, and it was difficult to see which mode the instrument was in.
Both groups disliked the switch key, and mentioned it just as frequently.
\begin{displayquote}[TS Subject]
    Switching from numbers to letters was hard, especially if I was trying to compensate for turbulence and was struggling at the time.
\end{displayquote}
\begin{displayquote}[VR Subject]
    I did not like how much extra work it was. It took so much extra focus that I forgot I was flying with the joystick
\end{displayquote}

Many subjects noted the familiarity of the Keypad design (Keypad, Familiar) and that having the buttons close together (Keypad, Buttons Proximal) as things they like about that design.
The familiarity was noted more often for the TS Group, but both were some of the more frequent comments within each group.

One comment about the Edgekey design that got more frequent mentions from the TS Group was that they found having the flight task in the middle of the display, centered between the buttons, was preferred (Edgekey, Centered Flight Task Better).
The subjects who chose the Edgekey as their preferred design nearly unanimously cited this as their reason for their preference\footnote{The one holdout did not explain why they preferred the Edgekey design.}.
The comments that fed into this category also included subjects who noted the difficulty of splitting their focus back and forth with the Keypad design.
Interestingly, two of the TS Group subjects noted that they would have found the Keypad easier if they had tactile feedback to guide their input.
This could suggest that the reason the VR Group subjects did not find the centered flight task advantageous is because with the tactile feedback of the 3D printed instruments they were able to keep visual focus on the left half of the screen in the Keypad design, thus not seeing benefit from the centering of the flight task display.
\begin{displayquote}[TS Subject]
    \textins{The Edgekey design} forced me to pay more attention to what I was typing, this wouldn't have been a problem if the keypad was a physical device that allowed me to locate the numbers and letters without looking, much like the dots on a computer keyboard.
\end{displayquote}
\begin{displayquote}[VR Subject]
    I like that the flight control was cent\textins{e}red, so you could see it even when you were looking at the buttons.
\end{displayquote}

The most notable exceptions to providing similar feedback between groups are the categories that relate to fatigue issues.
Four subjects in the TS group noted fatigue caused from using the joystick, yet none in the VR group did, despite using the same joystick setup, and seated in the same location.
The VR group did note more fatigue in their other arm that was used for the prompting task.
This fatigue seemed to be caused by the additional effort needed to have the hand tracker recognize the hand.
For example, one subject wrote:
\begin{displayquote}[VR Subject]
    My right wrist was somewhat fatigued.  Though I think this is mostly from positioning my hand for the simulator to recognize my input.
\end{displayquote}
Some of this additional effort was due to subjects learning to hold their prompting task hand ``hovering'' while waiting for the next prompt.
This was done to keep the hand in view of the hand tracker as when the hand leaves the field of view, the re-acquisition will slow down the entry of first button.
Many subjects organically learned this, and kept their arm in front of the instrument between prompts.

Similar to the fatigue issues being different, there were some comments that were due to the technology being used more-so than the designs themselves.
Obviously, the subjects who noted difficulty using the hand tracker, or the one subject who mentioned touchscreen issues, are specific to the simulator technology the used.
However, some of the other categories had comments that may have been indirectly caused by the different technologies and their limitations.
For example, some subjects noted the keypad design caused them to make more mistakes.
For the TS Group, this was due to the touchscreen being too responsive to the button presses:
\begin{displayquote}[TS Subject]
    \textins{S}ince I was able to go more quickly with this layout, I had more mistakes in the entry.
\end{displayquote}
One subject in VR who complained of more mistakes in the Keypad design identified a common problem caused by the hand tracker.
When the hand tracker was having registration issues it would sometimes mistakenly place the other fingers in the activation zone of the buttons underneath the one being targeted, causing multiple buttons to be pressed in a short period of time.
\begin{displayquote}[VR Subject]
    There's more unintended register since other fingers might trigger the buttons.
\end{displayquote}
Although only one subject noted this, it was observed happening to many subjects.
In fact, for the VR group, eight of the twelve subjects had the wrong button register within 200 milliseconds of the last button in the Keypad design.
In the other designs and groups this happened to only one or two subjects.


\section{Discussion}
\label{sec:de_discussion}

%The effect of Group on the tracking task performance was expected as the visual resolution of the tracking display was diminished in the VR head-mounted display.

Our results suggest that tasks or performance measures which are dominated by a cognitive portion, such as the prompt response time, provide similar results.
Tasks which rely on visual resolution or time pressured responses may not produce the same results between designs using the R3C system.
None of the effects reversed slope between designs, however, and the only change is in magnitude of the effect.
In fact, for both the number of prompts correct and the workload ratings, which had significant interaction effects, the use of the VR system amplified the effect of design within the groups from a marginally significant effect to a significant effect.

Many design evaluation studies would be concluded with both paper questionnaires as well as open interviews to receive the feedback from the subject.
Our experimental design avoided the use of the interview for two reasons.
First, since our subjects were not subject domain experts or experienced evaluators, we wanted to ensure that the prompting of the questions were consistent.
Second, the primary goal of the design feedback for this experiment was not to evaluate the designs, but rather to compare evaluations.
The use of a proctor interviewing the subjects could introduce accidental bias into the responses of the subjects.
This can often be useful when evaluating a new interface, for example, an interviewer could ask subjects about a flaw they had not mentioned yet to determine if they did not notice it or did not care about it.
However, in our case, we omitted this additional information to ensure no bias was introduced in the collection of their opinions.

Our initial motivation is to provide a virtual reality simulator to be used with an existing mockup.
In this experiment, we compared the performance of a touchscreen simulation to the use of our R3C system.
A touchscreen simulation is a higher fidelity simulation than what may exist during the mockup phase.
Our goal in this experiment was to show that the R3C system could provide the same feedback from a higher fidelity simulation.
This provides motivation for using the R3C system earlier in the design process (during the mockup phase).
Our results indicate that the VR/R3C system does provide the same feedback from evaluations, and dependent measures will transfer favorably except for time-pressured dependent measures.

This was a limited study of the utility of VR/R3C for design evaluation purposes.
The task and instrument design was kept simple in nature for this study in order to limit the amount of confounding variables as well as keep it easy to learn for the subject population.
Future studies could investigate this system in a more involved design study, with multiple instruments or designs, or more complex behavior in the cockpit.
At this point, it would become more essential to use subject domain experts (i.e.\ experienced pilots) in order to validate these results.

\section{Conclusion}

The motivation of this experiment was to determine the differences between using an R3C simulator system and a traditional simulator system to perform a design evaluation experiment.
We had two groups of subjects perform the same evaluation task on two different designs of a cockpit instrument, one group using the R3C system and the other a touchscreen system.
The evaluation task included two simultaneous tasks: a pitch disturbance tracking task with their left hand and a call and response prompting task with their right hand.
In addition to the quantitative performance measures of the task, subjects were asked for their feedback on the two designs at the conclusion of the experiment.

The results are summarized using their two independent variables: Group and Design.
Group, a between subjects factor, refers to the technology the subject used: either Virtual Reality/R3C (VR) or Touchscreen (TS).
Design is a within subjects factor, and is the instrument design the subject was evaluation: Edgekey or Keypad.

The VR Group had worse performance than the TS Group with the RMSE of the tracking task.
Subjects from both groups had a marginally significant difference in tracking task performance due to Design, with subjects performing better with the Keypad design.
It was also shown that, on control trials that had only the tracking task (no prompting task), the effect of Group was reduced to marginally significant.
The response time of the prompting task had no significant effect based on Group nor Design.
Neither the RMSE nor the response time had interaction effects between Group and Design.
This indicates that the differences in these two dependent measures would be found whether the evaluation was performed using the VR/R3C system or the touchscreen system.

The percent of correct prompts and NASA-TLX workload score did have a significant interaction effect between Group and Design.
The TS Group was able to complete significantly more prompts correctly overall than the VR Group, but the VR Group had a significant effect with Design and the TS Group only had a marginally significant effect.
The TLX scores for the VR Group showed a significant effect in Design, with subjects rating the Edgekey design over 30 points higher than the Keypad design (averages of 54.4 to 85.6, respectively).
However, like the percent of correct prompts, the TLX score was found to be only marginally significant for the TS group, rating the Keypad at 61.5 compared to the Edgekey at 69.2.
We attribute the rejection of the null interaction hypotheses to the increased workload of activating buttons in the R3C system coupled with the time pressures of the prompting task.

The results of the subjective feedback analysis found that there was no omission of major feedback items on the design of the two instruments from either group.
The only feedback comments that did not transfer were the fatigue issues, and technology-specific issues.
We did discover that some issues were mentioned at differing frequencies, which is to say, one group would have more subjects mention it than the other.
These results suggest that the use of the R3C system for receiving feedback from a design would be appropriate.
